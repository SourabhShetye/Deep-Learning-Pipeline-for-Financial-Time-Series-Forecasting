{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations and Imports \n",
    "!pip install tensorflow pandas pandas-ta pandas-datareader scikit-learn keras-tuner vaderSentiment -q\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import datetime\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas_ta as ta\n",
    "import pandas_datareader.data as web\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207225af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration \n",
    "CONFIG = {\n",
    "    \"alpha_vantage\": {\"key\": \"ALPHAVENTAGE_API_KEY\", \"symbol\": \"IBM\"},\n",
    "    \"data\": {\"start_date\": \"2010-01-01\", \"end_date\": datetime.date.today().strftime(\"%Y-%m-%d\"), \"target_column\": \"close\", \"window_size\": 30},\n",
    "    \"relative_strength\": {\"etf\": \"XLK\"},\n",
    "    \"sentiment\": {\"threshold\": 0.03},\n",
    "    \"walk_forward\": {\"train_days\": 500, \"test_days\": 90, \"step\": 120},\n",
    "    \"tuner\": {\"max_trials\": 8, \"epochs\": 15},\n",
    "    \"training\": {\"epochs\": 100, \"batch_size\": 32}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Function\n",
    "def create_advanced_feature_dataset(config):\n",
    "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"], output_format='pandas')\n",
    "    print(f\"Fetching data for {config['alpha_vantage']['symbol']} and {config['relative_strength']['etf']}...\")\n",
    "    df, _ = ts.get_daily(config[\"alpha_vantage\"][\"symbol\"], outputsize=\"full\")\n",
    "    df.sort_index(inplace=True)\n",
    "    df = df.loc[config[\"data\"][\"start_date\"]:config[\"data\"][\"end_date\"]]\n",
    "    df.columns = [col.split('. ')[1] for col in df.columns]\n",
    "\n",
    "    etf_df, _ = ts.get_daily(config['relative_strength']['etf'], outputsize=\"full\")\n",
    "    etf_df.sort_index(inplace=True)\n",
    "    etf_df.columns = [col.split('. ')[1] for col in etf_df.columns]\n",
    "    df[f\"{config['relative_strength']['etf']}_close\"] = etf_df['close']\n",
    "    df.ffill(inplace=True)\n",
    "    df['relative_strength'] = df['close'] / df[f\"{config['relative_strength']['etf']}_close\"]\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['price_change'] = df['close'].pct_change()\n",
    "    df['headline'] = df['price_change'].apply(lambda change: f\"Stock surges\" if change > config[\"sentiment\"][\"threshold\"] else (f\"Stock plummets\" if change < -config[\"sentiment\"][\"threshold\"] else f\"Stock stable\"))\n",
    "    df['sentiment'] = df['headline'].apply(lambda h: analyzer.polarity_scores(h)['compound'])\n",
    "\n",
    "    df.ta.rsi(length=14, append=True)\n",
    "    df.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "    df.ta.bbands(length=20, append=True)\n",
    "    df.ta.obv(append=True)\n",
    "    \n",
    "    df['rsi_oversold'] = np.where(df['RSI_14'] < 30, 1, 0)\n",
    "    df['rsi_overbought'] = np.where(df['RSI_14'] > 70, 1, 0)\n",
    "    macd_line, signal_line = df['MACD_12_26_9'], df['MACDs_12_26_9']\n",
    "    df['macd_bullish_cross'] = np.where((macd_line > signal_line) & (macd_line.shift(1) < signal_line.shift(1)), 1, 0)\n",
    "    \n",
    "    for lag in range(1, 4):\n",
    "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "        df[f'volume_lag_{lag}'] = df['volume'].shift(lag)\n",
    "    \n",
    "    df = df.drop(columns=['headline', 'price_change']).dropna().astype(np.float32)\n",
    "    print(f\"Feature engineering complete. Final dataset shape: {df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting Functions and Model Definition\n",
    "def create_windowed_data(data, window_size, target_col_index):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size, target_col_index])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model_for_tuning(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv1D(filters=hp.Int('conv_filters', 32, 128, 32), kernel_size=hp.Choice('kernel_size', [3, 5]), activation='relu', input_shape=(CONFIG[\"data\"][\"window_size\"], X_placeholder.shape[2])))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(GRU(units=hp.Int('gru_units', 40, 100, 20), return_sequences=True))\n",
    "    model.add(GRU(units=hp.Int('gru_units_2', 40, 100, 20)))\n",
    "    model.add(Dropout(hp.Float('dropout', 0.1, 0.5, 0.1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='mean_squared_error')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "master_df = create_advanced_feature_dataset(CONFIG)\n",
    "global X_placeholder\n",
    "X_placeholder = np.zeros((1, CONFIG[\"data\"][\"window_size\"], master_df.shape[1]))\n",
    "\n",
    "print(f\"\\n--- Tuning Single, Robust Model ---\")\n",
    "tuning_slice_df = master_df.iloc[:CONFIG[\"walk_forward\"][\"train_days\"]]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(tuning_slice_df)\n",
    "target_col_idx = tuning_slice_df.columns.get_loc(CONFIG[\"data\"][\"target_column\"])\n",
    "X_train_tuning, y_train_tuning = create_windowed_data(scaled_data, CONFIG[\"data\"][\"window_size\"], target_col_idx)\n",
    "X_placeholder = X_train_tuning\n",
    "\n",
    "tuner = kt.RandomSearch(build_model_for_tuning, objective='val_loss', max_trials=CONFIG[\"tuner\"][\"max_trials\"], executions_per_trial=1, directory='tuner_dir_final', project_name='final_model')\n",
    "tuner.search(X_train_tuning, y_train_tuning, epochs=CONFIG[\"tuner\"][\"epochs\"], validation_split=0.2, verbose=1)\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"--- Best Hyperparameters Found ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Walk-Forward Validation Function\n",
    "def run_single_model_walk_forward(df, config, best_hps):\n",
    "    all_actuals, all_predictions, all_dates = [], [], []\n",
    "    target_col = config[\"data\"][\"target_column\"]\n",
    "    print(\"\\n--- Starting Single-Model Walk-Forward Validation ---\")\n",
    "    for i in range(0, len(df) - config[\"walk_forward\"][\"train_days\"] - config[\"walk_forward\"][\"test_days\"] + 1, config[\"walk_forward\"][\"step\"]):\n",
    "        train_slice = df.iloc[i : i + config[\"walk_forward\"][\"train_days\"]]\n",
    "        test_slice = df.iloc[i + config[\"walk_forward\"][\"train_days\"] : i + config[\"walk_forward\"][\"train_days\"] + config[\"walk_forward\"][\"test_days\"]]\n",
    "        print(f\"\\n--- FOLD: Training on {train_slice.index.min().date()} to {train_slice.index.max().date()} ---\")\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_train = scaler.fit_transform(train_slice)\n",
    "        X_train, y_train = create_windowed_data(scaled_train, config[\"data\"][\"window_size\"], train_slice.columns.get_loc(target_col))\n",
    "        \n",
    "        model = build_model_for_tuning(best_hps)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, epochs=config[\"training\"][\"epochs\"], batch_size=config[\"training\"][\"batch_size\"], validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "        \n",
    "        history = train_slice.iloc[-config[\"data\"][\"window_size\"]:]\n",
    "        fold_predictions = []\n",
    "        for j in range(len(test_slice)):\n",
    "            current_day = test_slice.iloc[j:j+1]\n",
    "            prediction_input = pd.concat([history, current_day])\n",
    "            scaled_input = scaler.transform(prediction_input)\n",
    "            X_pred = np.array([scaled_input])\n",
    "            scaled_pred = model.predict(X_pred, verbose=0)\n",
    "            dummy = np.zeros((1, len(df.columns))); dummy[0, df.columns.get_loc(target_col)] = scaled_pred\n",
    "            prediction = scaler.inverse_transform(dummy)[0, df.columns.get_loc(target_col)]\n",
    "            fold_predictions.append(prediction)\n",
    "            history = pd.concat([history.iloc[1:], current_day])\n",
    "\n",
    "        all_predictions.extend(fold_predictions)\n",
    "        all_actuals.extend(test_slice[target_col].values)\n",
    "        all_dates.extend(test_slice.index)\n",
    "    return pd.DataFrame({'Actual': all_actuals, 'Predicted': all_predictions}, index=pd.to_datetime(all_dates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execution and Final Analysis\n",
    "results_df = run_single_model_walk_forward(master_df, CONFIG, best_hps)\n",
    "if not results_df.empty:\n",
    "    results_df.dropna(inplace=True)\n",
    "    actual_direction = np.diff(results_df['Actual']) > 0\n",
    "    predicted_direction = np.diff(results_df['Predicted']) > 0\n",
    "    final_directional_accuracy = np.mean(actual_direction == predicted_direction) * 100\n",
    "    mae = np.mean(np.abs(results_df['Predicted'] - results_df['Actual']))\n",
    "    rmse = np.sqrt(np.mean((results_df['Predicted'] - results_df['Actual'])**2))\n",
    "    print(f\"\\n--- FINAL BACKTEST RESULTS ---\")\n",
    "    print(f\"Robust Directional Accuracy: {final_directional_accuracy:.2f}%\")\n",
    "    print(f\"Mean Absolute Error (MAE): ${mae:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): ${rmse:.2f}\")\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig = figure(figsize=(25, 8), dpi=80)\n",
    "    plt.plot(results_df['Actual'], label=\"Actual Prices\", color=\"blue\", linewidth=2)\n",
    "    plt.plot(results_df['Predicted'], label=\"Walk-Forward Predictions\", color=\"red\", linestyle='--', alpha=0.8)\n",
    "    plt.title(f\"Single-Model Walk-Forward Validation for {CONFIG['alpha_vantage']['symbol']}\", fontsize=16)\n",
    "    plt.xlabel(\"Date\", fontsize=12); plt.ylabel(\"Price\", fontsize=12); plt.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c94b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
